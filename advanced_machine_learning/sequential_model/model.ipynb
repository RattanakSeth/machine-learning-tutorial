{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "from tensorflow.python.keras.layers import Dense, Activation, Dropout, SimpleRNN\n",
    "from tensorflow.python.keras.models import Sequential, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['num' 'weekday' 'hour']\n",
      " ['0' '5' '17']\n",
      " ['1' '5' '17']\n",
      " ...\n",
      " ['6' '2' '15']\n",
      " ['6' '2' '15']\n",
      " ['6' '2' '15']]\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "import pandas as pd\n",
    "df = pd.read_csv('data/bike_rnn.csv', sep=',', header=None)\n",
    "print(df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes = []\n",
    "def data_bike_num(path_to_dataset='data/bike_rnn.csv',\n",
    "                           sequence_length=20,\n",
    "                           ratio=1.0):\n",
    "    max_values = ratio * 45949\n",
    "    with open(path_to_dataset) as f:\n",
    "        data = csv.reader(f, delimiter=\",\")\n",
    "        next(data, None)  # skip the headers\n",
    "        nb_of_values = 0\n",
    "        for line in data:\n",
    "            try:\n",
    "                bikes.append(float(line[0]))\n",
    "                nb_of_values += 1\n",
    "            except ValueError:\n",
    "                pass\n",
    "            if nb_of_values >= max_values:\n",
    "                break\n",
    "    print (\"Data loaded from csv. Formatting...\")\n",
    "    print(len(bikes))\n",
    "\n",
    "    result = []\n",
    "    for index in range(len(bikes) - sequence_length):\n",
    "        result.append(bikes[index: index + sequence_length])\n",
    "    result = np.array(result)  # shape (2049230, 50)\n",
    "    result_mean = result.mean()\n",
    "    result -= result_mean\n",
    "    print(\"Shift: \", result_mean)\n",
    "    print (\"Data: \", result.shape)\n",
    "    row = int(round(0.95 * result.shape[0]))\n",
    "    train = result[:row, :]\n",
    "    np.random.shuffle(train)\n",
    "    X_train = train[:, :-1]\n",
    "    y_train = train[:, -1]\n",
    "    X_test = result[row:, :-1] # 2297\n",
    "    y_test = result[row:, -1]\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "    return [X_train, y_train, X_test, y_test, result_mean]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from csv. Formatting...\n",
      "45949\n",
      "Shift:  6.997708419517081\n",
      "Data:  (45929, 20)\n",
      "\n",
      "Data Loaded. Compiling...\n",
      "\n",
      "Compilation Time :  0.005113840103149414\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.python.distribute.input_lib' has no attribute 'DistributedDatasetInterface'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m (\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mData Loaded. Compiling...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     31\u001b[39m model = build_model()\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m predicted = model.predict(X_test)\n\u001b[32m     36\u001b[39m predicted = np.reshape(predicted, (predicted.size,))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py312/lib/python3.12/site-packages/tensorflow/python/keras/engine/training.py:1137\u001b[39m, in \u001b[36mModel.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[39m\n\u001b[32m   1131\u001b[39m   \u001b[38;5;28mself\u001b[39m._cluster_coordinator = cluster_coordinator.ClusterCoordinator(\n\u001b[32m   1132\u001b[39m       \u001b[38;5;28mself\u001b[39m.distribute_strategy)\n\u001b[32m   1134\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.distribute_strategy.scope(), \\\n\u001b[32m   1135\u001b[39m      training_utils.RespectCompiledTrainableState(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m   1136\u001b[39m   \u001b[38;5;66;03m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1137\u001b[39m   data_handler = \u001b[43mdata_adapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_data_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1138\u001b[39m \u001b[43m      \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1139\u001b[39m \u001b[43m      \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1140\u001b[39m \u001b[43m      \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[43m      \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1142\u001b[39m \u001b[43m      \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1143\u001b[39m \u001b[43m      \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1144\u001b[39m \u001b[43m      \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m      \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m      \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m      \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m      \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m      \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_steps_per_execution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1153\u001b[39m   \u001b[38;5;66;03m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[32m   1154\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module.CallbackList):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py312/lib/python3.12/site-packages/tensorflow/python/keras/engine/data_adapter.py:1397\u001b[39m, in \u001b[36mget_data_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1395\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(kwargs[\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33m_cluster_coordinator\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   1396\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(*args, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1397\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataHandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py312/lib/python3.12/site-packages/tensorflow/python/keras/engine/data_adapter.py:1151\u001b[39m, in \u001b[36mDataHandler.__init__\u001b[39m\u001b[34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[39m\n\u001b[32m   1148\u001b[39m   \u001b[38;5;28mself\u001b[39m._steps_per_execution = steps_per_execution\n\u001b[32m   1149\u001b[39m   \u001b[38;5;28mself\u001b[39m._steps_per_execution_value = steps_per_execution.numpy().item()\n\u001b[32m-> \u001b[39m\u001b[32m1151\u001b[39m adapter_cls = \u001b[43mselect_data_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[38;5;28mself\u001b[39m._adapter = adapter_cls(\n\u001b[32m   1153\u001b[39m     x,\n\u001b[32m   1154\u001b[39m     y,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1163\u001b[39m     distribution_strategy=distribute_lib.get_strategy(),\n\u001b[32m   1164\u001b[39m     model=model)\n\u001b[32m   1166\u001b[39m strategy = distribute_lib.get_strategy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py312/lib/python3.12/site-packages/tensorflow/python/keras/engine/data_adapter.py:987\u001b[39m, in \u001b[36mselect_data_adapter\u001b[39m\u001b[34m(x, y)\u001b[39m\n\u001b[32m    985\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mselect_data_adapter\u001b[39m(x, y):\n\u001b[32m    986\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Selects a data adapter than can handle a given x and y.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m987\u001b[39m   adapter_cls = [\u001b[38;5;28mcls\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ALL_ADAPTER_CLS \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcan_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[32m    988\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m adapter_cls:\n\u001b[32m    989\u001b[39m     \u001b[38;5;66;03m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[32m    990\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFailed to find data adapter that can handle \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33minput: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    993\u001b[39m             _type_name(x), _type_name(y)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py312/lib/python3.12/site-packages/tensorflow/python/keras/engine/data_adapter.py:706\u001b[39m, in \u001b[36mDatasetAdapter.can_handle\u001b[39m\u001b[34m(x, y)\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    704\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcan_handle\u001b[39m(x, y=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    705\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(x, (data_types.DatasetV1, data_types.DatasetV2)) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m706\u001b[39m           \u001b[43m_is_distributed_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py312/lib/python3.12/site-packages/tensorflow/python/keras/engine/data_adapter.py:1696\u001b[39m, in \u001b[36m_is_distributed_dataset\u001b[39m\u001b[34m(ds)\u001b[39m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_is_distributed_dataset\u001b[39m(ds):\n\u001b[32m-> \u001b[39m\u001b[32m1696\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ds, \u001b[43minput_lib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDistributedDatasetInterface\u001b[49m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'tensorflow.python.distribute.input_lib' has no attribute 'DistributedDatasetInterface'"
     ]
    }
   ],
   "source": [
    "# Function to build the RNN-based model\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    layers = [1, 50, 100, 1]\n",
    "    model.add(SimpleRNN(\n",
    "        layers[1],\n",
    "        input_shape=(None, layers[0]),\n",
    "        return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(SimpleRNN(\n",
    "        layers[2],\n",
    "        return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(layers[3]))\n",
    "    model.add(Activation(\"linear\"))\n",
    "    start = time.time()\n",
    "    model.compile(loss=\"mse\", optimizer=\"rmsprop\", metrics=['mae', 'mape'])\n",
    "    print (\"Compilation Time : \", time.time() - start)\n",
    "    return model\n",
    "\n",
    "# We are now ready to train the model we defined on our data.\n",
    "global_start_time = time.time()\n",
    "epochs = 2\n",
    "ratio = 1\n",
    "sequence_length = 20\n",
    "path_to_dataset = 'data/bike_rnn.csv'\n",
    "X_train, y_train, X_test, y_test, result_mean = data_bike_num(\n",
    "    path_to_dataset, sequence_length, ratio)\n",
    "\n",
    "print ('\\nData Loaded. Compiling...\\n')\n",
    "model = build_model()\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=512, epochs=epochs, validation_split=0.05)\n",
    "predicted = model.predict(X_test)\n",
    "predicted = np.reshape(predicted, (predicted.size,))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
