{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/rattanak/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/rattanak/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/rattanak/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.util import ngrams\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Gutenberg Corpus (or replace with your own text corpus)\n",
    "# For demonstration, combining a few texts from the Gutenberg corpus\n",
    "corpus = (\n",
    "    # gutenberg.raw('crawler/data/wikipedia.txt')\n",
    "    gutenberg.raw('austen-emma.txt') +\n",
    "    gutenberg.raw('austen-sense.txt') +\n",
    "    gutenberg.raw('bible-kjv.txt')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Split the Corpus into Subsets\n",
    "# Shuffle the sentences to ensure random sampling\n",
    "corpus = corpus.replace('--', '').replace('``', '').replace(\"''\", '').replace('`', '')\n",
    "\n",
    "sentences = corpus.split('\\n')\n",
    "random.shuffle(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate subset sizes\n",
    "total_sentences = len(sentences)\n",
    "train_size = int(0.7 * total_sentences)\n",
    "val_size = int(0.1 * total_sentences)\n",
    "test_size = total_sentences - train_size - val_size\n",
    "\n",
    "# Create subsets\n",
    "train_sentences = sentences[:train_size]\n",
    "val_sentences = sentences[train_size:train_size + val_size]\n",
    "test_sentences = sentences[train_size + val_size:]\n",
    "\n",
    "# Combine sentences back into text for tokenization\n",
    "train_text = ' '.join(train_sentences)\n",
    "val_text = ' '.join(val_sentences)\n",
    "test_text = ' '.join(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', 'of them, in earnest conversation with a very fashionable', '', 'without blemish: 28:20 And their meat offering shall be of flour', 'cometh to God must believe that he is, and that he is a rewarder of', 'loose the sackcloth from off thy loins, and put off thy shoe from thy', '']\n",
      "Length:  131425\n"
     ]
    }
   ],
   "source": [
    "print(train_sentences[:8]),\n",
    "print(\"Length: \", total_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the Text\n",
    "train_tokens = word_tokenize(train_text.lower())\n",
    "val_tokens = word_tokenize(val_text.lower())\n",
    "test_tokens = word_tokenize(test_text.lower())\n",
    "# train_tokens = word_tokenize(train_text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Training Tokens: 891920\n",
      "Training Tokens After Limiting Vocabulary: 891920\n",
      "Example Training Tokens: ['of', 'them', ',', 'in', 'earnest', 'conversation', 'with', 'a', 'very', '<UNK>', 'without', 'blemish', ':', '28:20', 'and', 'their', 'meat', 'offering', 'shall', 'be', 'of', 'flour', 'cometh', 'to', 'god', 'must', 'believe', 'that', 'he', 'is', ',', 'and', 'that', 'he', 'is', 'a', '<UNK>', 'of', 'loose', 'the', 'sackcloth', 'from', 'off', 'thy', 'loins', ',', 'and', 'put', 'off', 'thy']\n"
     ]
    }
   ],
   "source": [
    "# Limit Vocabulary Size and Replace Rare Tokens with <UNK>\n",
    "# Define maximum vocabulary size\n",
    "MAX_VOCAB_SIZE = 5000\n",
    "\n",
    "# Count word frequencies in the training set\n",
    "word_counts = Counter(train_tokens)\n",
    "\n",
    "# Select the most common words for the vocabulary\n",
    "vocab = {word for word, _ in word_counts.most_common(MAX_VOCAB_SIZE)}\n",
    "\n",
    "# Replace rare tokens with <UNK>\n",
    "def replace_with_unk(tokens, vocab):\n",
    "    return [token if token in vocab else \"<UNK>\" for token in tokens]\n",
    "\n",
    "train_tokens_limited = replace_with_unk(train_tokens, vocab)\n",
    "val_tokens_limited = replace_with_unk(val_tokens, vocab)\n",
    "test_tokens_limited = replace_with_unk(test_tokens, vocab)\n",
    "\n",
    "# Results\n",
    "print(f\"Original Training Tokens: {len(train_tokens)}\")\n",
    "print(f\"Training Tokens After Limiting Vocabulary: {len(train_tokens_limited)}\")\n",
    "print(f\"Example Training Tokens: {train_tokens_limited[:50]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add <s> (start) and </s> (end) tokens for sentence boundaries\n",
    "\"\"\"\n",
    "A 4-gram model generates n-grams of length 4. For the first token in a sentence, there are no preceding words. \n",
    "To address this, multiple <s> tokens are added to represent the missing preceding words, ensuring that every word in the sentence can appear as part of a valid 4-gram.\n",
    "\"\"\"\n",
    "train_tokens_limited = [\"<s>\", \"<s>\", \"<s>\"] + train_tokens_limited + [\"</s>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '<s>', '<s>', 'of', 'them', ',', 'in', 'earnest', 'conversation', 'with', 'a', 'very', '<UNK>', 'without', 'blemish', ':', '28:20', 'and', 'their', 'meat', 'offering', 'shall', 'be', 'of', 'flour', 'cometh', 'to', 'god', 'must', 'believe', 'that', 'he', 'is', ',', 'and', 'that', 'he', 'is', 'a', '<UNK>', 'of', 'loose', 'the', 'sackcloth', 'from', 'off', 'thy', 'loins', ',', 'and', 'put', 'off', 'thy', '<UNK>', 'from', 'thy', 'fountain', 'of', 'living', 'waters', ',', 'and', 'hewed', 'them', 'out', '<UNK>', ',', 'broken', 'man', 'that', 'hath', 'left', 'house', ',', 'or', 'brethren', ',', 'or', 'sisters', ',', 'or', 'father', ',', 'or', 'candlestick', ',', 'and', 'also', 'for', 'the', 'lamps', 'thereof', ',', 'according', 'to', 'the', 'use', 'of', '<UNK>', 'who']\n"
     ]
    }
   ],
   "source": [
    "print(train_tokens_limited[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count n-grams\n",
    "unigram_counts = Counter(ngrams(train_tokens_limited, 1))\n",
    "bigram_counts = Counter(ngrams(train_tokens_limited, 2))\n",
    "trigram_counts = Counter(ngrams(train_tokens_limited, 3))\n",
    "fourgram_counts = Counter(ngrams(train_tokens_limited, 4))\n",
    "\n",
    "# Total unigrams (needed for probabilities)\n",
    "total_unigrams = sum(unigram_counts.values())\n",
    "\n",
    "# Vocabulary size (for smoothing)\n",
    "vocab_size = len(set(train_tokens_limited))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'communed',\n",
       " 'satan',\n",
       " \"''\",\n",
       " 'death',\n",
       " '12:11',\n",
       " 'governors',\n",
       " '12:1',\n",
       " 'ahaz',\n",
       " 'praised',\n",
       " 'bates',\n",
       " 'a-year',\n",
       " 'borders',\n",
       " 'own',\n",
       " 'galilee',\n",
       " 'fulfil',\n",
       " 'poverty',\n",
       " 'levi',\n",
       " 'seemeth',\n",
       " 'distance',\n",
       " 'everything',\n",
       " '17:8',\n",
       " 'set',\n",
       " 'language',\n",
       " 'difference',\n",
       " 'bowels',\n",
       " 'owed',\n",
       " 'confess',\n",
       " 'agitated',\n",
       " 'seventy',\n",
       " 'entering',\n",
       " 'cities',\n",
       " 'wickedly',\n",
       " '16:20',\n",
       " 'refuse',\n",
       " 'shocked',\n",
       " '5:27',\n",
       " 'perceiving',\n",
       " 'sing',\n",
       " 'sinner',\n",
       " 'choose',\n",
       " 'inner',\n",
       " 'tomorrow',\n",
       " 'jedaiah',\n",
       " '19:11',\n",
       " 'treated',\n",
       " 'established',\n",
       " 'singing',\n",
       " 'encouragement',\n",
       " 'fish',\n",
       " '25:8',\n",
       " 'punished',\n",
       " 'fellows',\n",
       " 'persuaded',\n",
       " 'rend',\n",
       " 'concern',\n",
       " 'direct',\n",
       " '21:17',\n",
       " 'miserable',\n",
       " 'contend',\n",
       " '13:32',\n",
       " 'knowing',\n",
       " 'publicans',\n",
       " 'siege',\n",
       " '12:4',\n",
       " '3:13',\n",
       " 'on',\n",
       " 'avenged',\n",
       " 'sojourn',\n",
       " 'heavy',\n",
       " 'among',\n",
       " '16:10',\n",
       " 'nineveh',\n",
       " 'lamentation',\n",
       " 'gave',\n",
       " 'conscience',\n",
       " '18:21',\n",
       " 'true',\n",
       " 'princes',\n",
       " 'influence',\n",
       " 'attentive',\n",
       " 'fowls',\n",
       " 'sepulchre',\n",
       " 'snare',\n",
       " '18:14',\n",
       " '8:5',\n",
       " 'useful',\n",
       " '16:25',\n",
       " 'shot',\n",
       " 'lusts',\n",
       " 'breaking',\n",
       " 'nothing',\n",
       " 'husband',\n",
       " 'innocent',\n",
       " 'valleys',\n",
       " 'naomi',\n",
       " 'thinking',\n",
       " 'drop',\n",
       " 'forward',\n",
       " 'wherefore',\n",
       " 'advantage',\n",
       " 'glorified',\n",
       " 'oxen',\n",
       " 'imagination',\n",
       " 'power',\n",
       " 'even',\n",
       " 'betrayed',\n",
       " 'thereon',\n",
       " 'hole',\n",
       " 'nethaniah',\n",
       " 'persecuted',\n",
       " 'connected',\n",
       " 'concealed',\n",
       " 'hazael',\n",
       " 'uneasy',\n",
       " 'hint',\n",
       " 'vexed',\n",
       " 'subdued',\n",
       " 'saul',\n",
       " 'goddard',\n",
       " 'desired',\n",
       " '6:16',\n",
       " 'bound',\n",
       " '3:4',\n",
       " 'nigh',\n",
       " 'jephunneh',\n",
       " 'esteemed',\n",
       " 'breasts',\n",
       " 'savour',\n",
       " '11:11',\n",
       " 'northward',\n",
       " 'clay',\n",
       " 'civilities',\n",
       " 'shock',\n",
       " 'firstfruits',\n",
       " 'instantly',\n",
       " 'fulfilled',\n",
       " 'style',\n",
       " 'form',\n",
       " 'prisoner',\n",
       " 'trespass',\n",
       " 'independence',\n",
       " 'drive',\n",
       " 'lover',\n",
       " 'respectable',\n",
       " 'amalekites',\n",
       " 'possibly',\n",
       " 'father',\n",
       " '1:15',\n",
       " 'again',\n",
       " 'believed',\n",
       " 'destroy',\n",
       " '33:7',\n",
       " '21:2',\n",
       " 'absalom',\n",
       " 'thinks',\n",
       " 'barren',\n",
       " 'counsel',\n",
       " 'teach',\n",
       " '8:10',\n",
       " 'preaching',\n",
       " '2:9',\n",
       " '10:28',\n",
       " 'vanity',\n",
       " 'chance',\n",
       " 'inquire',\n",
       " 'attach',\n",
       " 'fasting',\n",
       " 'wast',\n",
       " 'pleasures',\n",
       " 'lo',\n",
       " 'fourteenth',\n",
       " '11:32',\n",
       " 'surprized',\n",
       " 'graven',\n",
       " 'than',\n",
       " 'beareth',\n",
       " 'grief',\n",
       " 'sockets',\n",
       " '11:33',\n",
       " 'as',\n",
       " 'sides',\n",
       " 'villages',\n",
       " 'rational',\n",
       " 'straight',\n",
       " 'while',\n",
       " 'commit',\n",
       " 'strengthened',\n",
       " 'pound',\n",
       " 'sanctified',\n",
       " 'ran',\n",
       " '8:23',\n",
       " 'interesting',\n",
       " 'lovers',\n",
       " 'rejoice',\n",
       " 'valley',\n",
       " 'usury',\n",
       " 'laban',\n",
       " 'elam',\n",
       " 'importance',\n",
       " 'golden',\n",
       " 'shallum',\n",
       " 'attention',\n",
       " 'edomites',\n",
       " '23:10',\n",
       " 'pit',\n",
       " 'rent',\n",
       " 'guided',\n",
       " 'churches',\n",
       " 'joash',\n",
       " 'amiss',\n",
       " 'voice',\n",
       " 'whatever',\n",
       " 'private',\n",
       " 'parting',\n",
       " '9:9',\n",
       " 'greet',\n",
       " 'loves',\n",
       " 'fornication',\n",
       " 'swear',\n",
       " 'custom',\n",
       " 'burnt',\n",
       " 'inclination',\n",
       " 'leavened',\n",
       " 'kindly',\n",
       " 'join',\n",
       " 'nurse',\n",
       " '9:25',\n",
       " '8:3',\n",
       " '3:24',\n",
       " 'hallow',\n",
       " '20:13',\n",
       " 'bathe',\n",
       " 'ner',\n",
       " 'expectation',\n",
       " 'firstborn',\n",
       " 'well',\n",
       " 'trying',\n",
       " '7:22',\n",
       " 'greatest',\n",
       " 'lambs',\n",
       " 'views',\n",
       " 'rod',\n",
       " '9:14',\n",
       " 'ships',\n",
       " '3:10',\n",
       " '7:26',\n",
       " 'hungry',\n",
       " 'dwellings',\n",
       " 'faithfulness',\n",
       " 'saddled',\n",
       " 'fed',\n",
       " 'charger',\n",
       " 'should',\n",
       " 'bade',\n",
       " 'seems',\n",
       " 'iniquities',\n",
       " 'residue',\n",
       " 'abode',\n",
       " 'trust',\n",
       " '7:2',\n",
       " '7:5',\n",
       " 'feasts',\n",
       " '14:19',\n",
       " 'escaped',\n",
       " 'mention',\n",
       " 'anointing',\n",
       " 'fill',\n",
       " '8:19',\n",
       " 'purple',\n",
       " 'fruitful',\n",
       " 'armour',\n",
       " 'assurance',\n",
       " 'window',\n",
       " 'spoons',\n",
       " 'ear',\n",
       " 'ephah',\n",
       " 'ford',\n",
       " '27:18',\n",
       " 'beyond',\n",
       " '8:4',\n",
       " 'wax',\n",
       " 'nine',\n",
       " 'hanged',\n",
       " 'fully',\n",
       " 'to-day',\n",
       " 'synagogue',\n",
       " 'produce',\n",
       " 'multiply',\n",
       " 'eligible',\n",
       " 'beauty',\n",
       " 'conscious',\n",
       " 'reason',\n",
       " 'esaias',\n",
       " 'crucify',\n",
       " 'wouldest',\n",
       " '18:24',\n",
       " 'pretty',\n",
       " '21:20',\n",
       " 'fifteen',\n",
       " 'settled',\n",
       " 'real',\n",
       " 'uncircumcision',\n",
       " 'thirsty',\n",
       " '23:18',\n",
       " 'testified',\n",
       " 'directly',\n",
       " 'vile',\n",
       " 'come',\n",
       " 'ordinances',\n",
       " 'rank',\n",
       " 'asaph',\n",
       " '11:14',\n",
       " 'pitied',\n",
       " '12:15',\n",
       " 'setteth',\n",
       " 'lion',\n",
       " 'lane',\n",
       " 'sole',\n",
       " 'thin',\n",
       " 'eighteen',\n",
       " 'seir',\n",
       " 'impatient',\n",
       " 'admit',\n",
       " 'nebuchadrezzar',\n",
       " 'guests',\n",
       " 'parts',\n",
       " 'calamity',\n",
       " 'still',\n",
       " '6:21',\n",
       " 'points',\n",
       " 'emma',\n",
       " '29:3',\n",
       " 'satisfy',\n",
       " '4:32',\n",
       " 'disease',\n",
       " 'stream',\n",
       " 'canst',\n",
       " 'bare',\n",
       " '16:9',\n",
       " 'reflections',\n",
       " '10:12',\n",
       " 'heir',\n",
       " 'youth',\n",
       " '28:20',\n",
       " 'failed',\n",
       " 'excellency',\n",
       " 'consent',\n",
       " 'comforted',\n",
       " 'row',\n",
       " '4:24',\n",
       " 'nash',\n",
       " 'large',\n",
       " 'egypt',\n",
       " 'waves',\n",
       " 'clear',\n",
       " 'moving',\n",
       " 'many',\n",
       " 'gadites',\n",
       " 'dare',\n",
       " 'rich',\n",
       " 'removed',\n",
       " 'counsellors',\n",
       " 'loins',\n",
       " 'increase',\n",
       " 'harlot',\n",
       " 'covering',\n",
       " 'stirred',\n",
       " 'heareth',\n",
       " '22:15',\n",
       " 'wants',\n",
       " '24:18',\n",
       " 'couple',\n",
       " 'exercise',\n",
       " 'myself',\n",
       " 'iron',\n",
       " 'calf',\n",
       " 'live',\n",
       " 'divided',\n",
       " '9:26',\n",
       " 'mrs',\n",
       " 'moab',\n",
       " '20:1',\n",
       " 'giants',\n",
       " 'short',\n",
       " 'wretched',\n",
       " 'shout',\n",
       " 'posts',\n",
       " 'bear',\n",
       " 'workers',\n",
       " 'saviour',\n",
       " 'depend',\n",
       " 'age',\n",
       " 'hour',\n",
       " 'wheels',\n",
       " '26:8',\n",
       " 'polluted',\n",
       " 'pure',\n",
       " 'profane',\n",
       " 'paying',\n",
       " 'hananiah',\n",
       " 'smitten',\n",
       " '16:21',\n",
       " 'watch',\n",
       " 'lieth',\n",
       " 'working',\n",
       " '5:14',\n",
       " 'contempt',\n",
       " 'habitation',\n",
       " '10:2',\n",
       " 'pestilence',\n",
       " 'hated',\n",
       " 'forsook',\n",
       " 'defiled',\n",
       " 'fierce',\n",
       " 'companies',\n",
       " 'taketh',\n",
       " 'turned',\n",
       " 'it',\n",
       " 'achish',\n",
       " '1:30',\n",
       " 'persons',\n",
       " 'through',\n",
       " 'deceitful',\n",
       " 'resolved',\n",
       " 'rings',\n",
       " '27:5',\n",
       " 'approaching',\n",
       " 'eight',\n",
       " 'repair',\n",
       " '13:4',\n",
       " 'vows',\n",
       " 'means',\n",
       " 'eaten',\n",
       " 'youngest',\n",
       " 'seeming',\n",
       " 'bars',\n",
       " 'burned',\n",
       " 'timber',\n",
       " 'midian',\n",
       " 'altogether',\n",
       " '30:15',\n",
       " 'dominion',\n",
       " 'lies',\n",
       " 'grateful',\n",
       " 'weather',\n",
       " 'heed',\n",
       " 'wished',\n",
       " 'fourteen',\n",
       " 'excessively',\n",
       " 'ahimelech',\n",
       " 'rooted',\n",
       " 'certainty',\n",
       " 'hallowed',\n",
       " 'character',\n",
       " 'good-will',\n",
       " 'affair',\n",
       " 'certain',\n",
       " 'troublesome',\n",
       " '2:24',\n",
       " 'persuade',\n",
       " 'denying',\n",
       " '8:8',\n",
       " 'catch',\n",
       " 'violent',\n",
       " 'receive',\n",
       " 'petition',\n",
       " 'time',\n",
       " 'find',\n",
       " 'presence',\n",
       " '8:27',\n",
       " '9:12',\n",
       " 'conclusion',\n",
       " 'dwelling',\n",
       " 'samson',\n",
       " 'dwelleth',\n",
       " 'full',\n",
       " 'humbled',\n",
       " 'laver',\n",
       " 'beasts',\n",
       " 'moon',\n",
       " 'marry',\n",
       " 'donwell',\n",
       " 'unclean',\n",
       " 'grant',\n",
       " 'infinitely',\n",
       " 'remain',\n",
       " 'better',\n",
       " 'boldly',\n",
       " 'sheep',\n",
       " 'seal',\n",
       " 'arrived',\n",
       " 'relieve',\n",
       " '18:15',\n",
       " 'jehoash',\n",
       " '20:29',\n",
       " 'health',\n",
       " 'peter',\n",
       " 'scriptures',\n",
       " 'borne',\n",
       " '11:21',\n",
       " '1:12',\n",
       " 'brightness',\n",
       " 'despise',\n",
       " 'lamps',\n",
       " 'inherit',\n",
       " 'white',\n",
       " 'visions',\n",
       " 'happier',\n",
       " 'step',\n",
       " 'is',\n",
       " '11:3',\n",
       " 'gather',\n",
       " 'became',\n",
       " 'beware',\n",
       " 'holiness',\n",
       " 'howbeit',\n",
       " 'driven',\n",
       " 'drink',\n",
       " 'hell',\n",
       " 'statute',\n",
       " 'condemned',\n",
       " 'civil',\n",
       " 'theirs',\n",
       " 'capable',\n",
       " 'therefore',\n",
       " 'miss',\n",
       " 'abbey',\n",
       " '14:28',\n",
       " 'loosed',\n",
       " 'bottles',\n",
       " '5:21',\n",
       " 'beam',\n",
       " '1:14',\n",
       " 'itself',\n",
       " 'subjects',\n",
       " 'ferrars',\n",
       " 'hiss',\n",
       " 'noon',\n",
       " 'passing',\n",
       " 'assurances',\n",
       " 'came',\n",
       " 'about',\n",
       " 'speakest',\n",
       " 'proper',\n",
       " 'peculiar',\n",
       " 'samaria',\n",
       " 'captain',\n",
       " 'truly',\n",
       " 'consume',\n",
       " 'grounds',\n",
       " 'despised',\n",
       " 'observation',\n",
       " '19:17',\n",
       " 'gold',\n",
       " 'goodness',\n",
       " '7:6',\n",
       " '21:1',\n",
       " 'locust',\n",
       " 'willoughby',\n",
       " 'boughs',\n",
       " 'dashwoods',\n",
       " '28:17',\n",
       " 'bring',\n",
       " 'cut',\n",
       " '6:12',\n",
       " 'liberal',\n",
       " 'heavenly',\n",
       " 'once',\n",
       " 'did',\n",
       " 'lifted',\n",
       " 'plainly',\n",
       " 'instruments',\n",
       " 'enough',\n",
       " 'shewing',\n",
       " 'heshbon',\n",
       " 'crucified',\n",
       " '22:16',\n",
       " 'liveth',\n",
       " 'mistake',\n",
       " '12:19',\n",
       " 'bestowed',\n",
       " 'have',\n",
       " 'furniture',\n",
       " 'afflicted',\n",
       " '1:29',\n",
       " 'baked',\n",
       " 'bitter',\n",
       " 'staff',\n",
       " 'secret',\n",
       " 'province',\n",
       " 'hot',\n",
       " 'complete',\n",
       " 'justice',\n",
       " 'pomegranates',\n",
       " 'pronounce',\n",
       " 'betray',\n",
       " 'curious',\n",
       " 'cleveland',\n",
       " 'sickness',\n",
       " 'ruin',\n",
       " 'likeness',\n",
       " 'beget',\n",
       " '24:20',\n",
       " 'whose',\n",
       " 'probable',\n",
       " 'ahitub',\n",
       " 'concluded',\n",
       " '27:9',\n",
       " '4:12',\n",
       " 'porch',\n",
       " 'event',\n",
       " '8:6',\n",
       " 'fault',\n",
       " 'expense',\n",
       " 'spoil',\n",
       " 'fine',\n",
       " 'red',\n",
       " 'ease',\n",
       " 'resist',\n",
       " '26:10',\n",
       " '7:24',\n",
       " 'rods',\n",
       " 'teeth',\n",
       " 'honoured',\n",
       " 'circumstances',\n",
       " 'forehead',\n",
       " 'suspect',\n",
       " 'meekness',\n",
       " 'longsuffering',\n",
       " '26:16',\n",
       " 'kingdom',\n",
       " 'before',\n",
       " 'served',\n",
       " 'mouths',\n",
       " 'january',\n",
       " 'eden',\n",
       " '8:2',\n",
       " 'inevitable',\n",
       " '9:5',\n",
       " 'jephthah',\n",
       " 'mount',\n",
       " 'whirlwind',\n",
       " '5:6',\n",
       " 'indignation',\n",
       " 'loveth',\n",
       " 'information',\n",
       " 'begun',\n",
       " '4:27',\n",
       " 'friends',\n",
       " 'describe',\n",
       " 'playing',\n",
       " 'regarded',\n",
       " '19:14',\n",
       " 'wheel',\n",
       " 'three',\n",
       " 'speak',\n",
       " 'witness',\n",
       " 'leprosy',\n",
       " 'education',\n",
       " 'favoured',\n",
       " 'devise',\n",
       " 'lighted',\n",
       " '7:27',\n",
       " 'jonadab',\n",
       " 'subjection',\n",
       " 'risk',\n",
       " 'benhadad',\n",
       " 'connection',\n",
       " 'nor',\n",
       " '6:10',\n",
       " 'sell',\n",
       " 'the',\n",
       " 'life',\n",
       " 'ways',\n",
       " 'proof',\n",
       " 'amusement',\n",
       " 'beard',\n",
       " 'wrath',\n",
       " 'john',\n",
       " '15:3',\n",
       " 'zerah',\n",
       " '15:31',\n",
       " 'vessel',\n",
       " '16:19',\n",
       " 'sarah',\n",
       " 'amongst',\n",
       " '12:18',\n",
       " 'softened',\n",
       " 'zerubbabel',\n",
       " 'accepted',\n",
       " 'birds',\n",
       " 'amazed',\n",
       " '22:18',\n",
       " 'willingly',\n",
       " 'willing',\n",
       " 'wonderful',\n",
       " 'taken',\n",
       " 'remove',\n",
       " 'approved',\n",
       " 'causeth',\n",
       " 'merari',\n",
       " 'turns',\n",
       " 'ashamed',\n",
       " \"n't\",\n",
       " 'interpretation',\n",
       " 'shewn',\n",
       " 'occurred',\n",
       " 'leaving',\n",
       " 'conceive',\n",
       " 'deep',\n",
       " '19:27',\n",
       " 'plant',\n",
       " 'snow',\n",
       " 'frank',\n",
       " 'dig',\n",
       " 'further',\n",
       " 'faces',\n",
       " 'horeb',\n",
       " 'disappointment',\n",
       " 'since',\n",
       " 'procured',\n",
       " 'issue',\n",
       " '9:31',\n",
       " 'niece',\n",
       " '1:16',\n",
       " 'that',\n",
       " 'overcome',\n",
       " '8:12',\n",
       " 'abide',\n",
       " 'root',\n",
       " 'nice',\n",
       " 'streets',\n",
       " '5:8',\n",
       " '21:26',\n",
       " '2:30',\n",
       " 'paths',\n",
       " 'entereth',\n",
       " '4:21',\n",
       " 'naturally',\n",
       " 'goodly',\n",
       " 'robbed',\n",
       " 'silver',\n",
       " 'sadducees',\n",
       " 'without',\n",
       " 'murderers',\n",
       " 'respect',\n",
       " 'bondage',\n",
       " 'continual',\n",
       " 'highway',\n",
       " 'rebelled',\n",
       " 'high',\n",
       " 'vulgar',\n",
       " 'beneath',\n",
       " '20:6',\n",
       " 'liar',\n",
       " 'introduced',\n",
       " 'enlarge',\n",
       " 'camest',\n",
       " 'nether',\n",
       " 'parcel',\n",
       " 'gifts',\n",
       " 'ladies',\n",
       " 'uttered',\n",
       " 'masters',\n",
       " '22:6',\n",
       " 'dress',\n",
       " '2:26',\n",
       " 'upper',\n",
       " 'bowl',\n",
       " 'wounds',\n",
       " 'voluntarily',\n",
       " 'candlesticks',\n",
       " 'evil',\n",
       " 'fro',\n",
       " 'probability',\n",
       " 'troop',\n",
       " 'between',\n",
       " 'swine',\n",
       " 'around',\n",
       " '11:20',\n",
       " 'belial',\n",
       " 'propriety',\n",
       " '5:10',\n",
       " 'westward',\n",
       " 'concubines',\n",
       " 'obed',\n",
       " 'vinegar',\n",
       " 'leaves',\n",
       " 'wroth',\n",
       " '27:2',\n",
       " '21:8',\n",
       " 'angel',\n",
       " 'double',\n",
       " 'ride',\n",
       " 'situation',\n",
       " 'whosoever',\n",
       " 'knowledge',\n",
       " 'rebellious',\n",
       " 'maker',\n",
       " 'palm',\n",
       " 'anoint',\n",
       " 'songs',\n",
       " 'putteth',\n",
       " 'abominable',\n",
       " 'enquired',\n",
       " 'constant',\n",
       " 'honest',\n",
       " 'cause',\n",
       " 'gomorrah',\n",
       " 'learned',\n",
       " 'bethshemesh',\n",
       " 'intreated',\n",
       " 'finally',\n",
       " '14:24',\n",
       " 'work',\n",
       " 'boys',\n",
       " 'defence',\n",
       " 'tree',\n",
       " 'doors',\n",
       " 'seventeen',\n",
       " 'clothes',\n",
       " 'sin',\n",
       " 'imagine',\n",
       " 'astray',\n",
       " 'maidservant',\n",
       " '10:31',\n",
       " 'independent',\n",
       " 'offend',\n",
       " 'glorious',\n",
       " 'barnabas',\n",
       " 'imputed',\n",
       " 'entreated',\n",
       " 'afterward',\n",
       " 'some',\n",
       " '4:2',\n",
       " '21:14',\n",
       " 'height',\n",
       " 'engage',\n",
       " 'understood',\n",
       " 'fashion',\n",
       " 'coles',\n",
       " 'avoid',\n",
       " 'carry',\n",
       " 'until',\n",
       " 'tower',\n",
       " 'devices',\n",
       " 'testimonies',\n",
       " '16:22',\n",
       " 'public',\n",
       " 'seats',\n",
       " 'quicken',\n",
       " 'maid',\n",
       " 'seated',\n",
       " '10:24',\n",
       " '11:12',\n",
       " 'another',\n",
       " 'few',\n",
       " '5:12',\n",
       " 'comfort',\n",
       " 'spend',\n",
       " 'preserved',\n",
       " 'planted',\n",
       " 'concealment',\n",
       " 'expected',\n",
       " 'pleased',\n",
       " '15:16',\n",
       " 'road',\n",
       " 'know',\n",
       " '33:1',\n",
       " 'variety',\n",
       " '4:3',\n",
       " '10:17',\n",
       " 'hand',\n",
       " 'censure',\n",
       " 'deliver',\n",
       " 'joel',\n",
       " 'pledge',\n",
       " 'sound',\n",
       " 'weight',\n",
       " 'with',\n",
       " 'armed',\n",
       " 'concerning',\n",
       " 'plucked',\n",
       " 'amalek',\n",
       " 'under',\n",
       " 'tenderness',\n",
       " '8:20',\n",
       " 'burning',\n",
       " 'supplication',\n",
       " '23:14',\n",
       " 'gates',\n",
       " 'fall',\n",
       " 'pots',\n",
       " 'testify',\n",
       " 'but',\n",
       " 'presently',\n",
       " 'attentions',\n",
       " '16:12',\n",
       " 'dost',\n",
       " 'naphtali',\n",
       " 'sidon',\n",
       " 'answering',\n",
       " 'trumpets',\n",
       " '26:18',\n",
       " 'keepeth',\n",
       " '24:17',\n",
       " 'appeared',\n",
       " '13:31',\n",
       " 'abednego',\n",
       " 'dung',\n",
       " 'threshingfloor',\n",
       " 'luck',\n",
       " 'layeth',\n",
       " '8:14',\n",
       " 'indulgence',\n",
       " '15:19',\n",
       " 'fetched',\n",
       " 'shepherd',\n",
       " 'completely',\n",
       " '11:35',\n",
       " 'dogs',\n",
       " 'pressed',\n",
       " 'terms',\n",
       " 'heifer',\n",
       " 'privately',\n",
       " 'pray',\n",
       " 'jeshua',\n",
       " 'firm',\n",
       " 'portion',\n",
       " 'relation',\n",
       " 'hearing',\n",
       " 'abominations',\n",
       " 'questions',\n",
       " 'throne',\n",
       " 'upright',\n",
       " 'reuben',\n",
       " 'shephatiah',\n",
       " '5:23',\n",
       " 'wait',\n",
       " '9:27',\n",
       " '10:7',\n",
       " 'falleth',\n",
       " '22:4',\n",
       " 'jeroboam',\n",
       " 'israelites',\n",
       " 'afford',\n",
       " '4:11',\n",
       " 'wherein',\n",
       " 'kine',\n",
       " 'liked',\n",
       " '1:11',\n",
       " 'self-command',\n",
       " 'picture',\n",
       " 'command',\n",
       " 'obadiah',\n",
       " '18:29',\n",
       " 'shoulders',\n",
       " 'expression',\n",
       " 'pervert',\n",
       " '4:28',\n",
       " 'prayed',\n",
       " 'housekeeper',\n",
       " 'wound',\n",
       " 'acquit',\n",
       " 'handmaid',\n",
       " '2:17',\n",
       " 'resentment',\n",
       " 'chiefly',\n",
       " '15:17',\n",
       " 'nose',\n",
       " 'abilities',\n",
       " 'wilt',\n",
       " 'tribulation',\n",
       " 'damsel',\n",
       " 'dragons',\n",
       " 'lucy',\n",
       " '16:6',\n",
       " '24:12',\n",
       " 'ever',\n",
       " 'though',\n",
       " '14:4',\n",
       " 'wages',\n",
       " 'eldest',\n",
       " 'month',\n",
       " 'wings',\n",
       " '18:19',\n",
       " 'anointed',\n",
       " 'anguish',\n",
       " 'nobody',\n",
       " 'overthrow',\n",
       " 'thick',\n",
       " 'imagined',\n",
       " 'price',\n",
       " 'sink',\n",
       " 'counted',\n",
       " 'sprinkle',\n",
       " 'rulers',\n",
       " 'verily',\n",
       " 'engagement',\n",
       " 'barley',\n",
       " ...}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gram = ('<s>', '<s>', '<s>', 'your')\n",
    "# print(len(gram), fourgram_counts[('<UNK>', ',', 'and', '<UNK>')])\n",
    "# fourgram_counts[gram],\n",
    "# # trigram_counts[gram[:3]]\n",
    "# fourgram_counts\n",
    "set(train_tokens_limited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Backoff Function\n",
    "def backoff_probability(ngram):\n",
    "    \"\"\"\n",
    "    Compute the probability of an n-gram using backoff (unsmoothed).\n",
    "    Falls back to lower n-grams if the higher-order n-gram is missing.\n",
    "    \"\"\"\n",
    "    # print(\"ngrams: \", ngram)\n",
    "    # 4-gram case\n",
    "    if len(ngram) == 4 and fourgram_counts[ngram] > 0:\n",
    "        return fourgram_counts[ngram] / trigram_counts[ngram[:3]]\n",
    "    \n",
    "    # 3-gram case\n",
    "    elif len(ngram) == 4 and trigram_counts[ngram[1:]] > 0:\n",
    "        return trigram_counts[ngram[1:]] / bigram_counts[ngram[1:3]]\n",
    "    \n",
    "    # 2-gram case\n",
    "    elif len(ngram) == 4 and bigram_counts[ngram[2:]] > 0:\n",
    "        return bigram_counts[ngram[2:]] / unigram_counts[(ngram[2],)]\n",
    "    \n",
    "    # Unigram case (fallback)\n",
    "    elif len(ngram) == 4 and unigram_counts[(ngram[3],)] > 0:\n",
    "        return unigram_counts[(ngram[3],)] / total_unigrams\n",
    "    \n",
    "    # If no probabilities available, return 0\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Your example sentence goes here.\n",
      "  ('<s>', '<s>', '<s>', 'your') -> 0.0019721411241316526\n",
      "  ('<s>', '<s>', 'your', 'example') -> 1.1211717590287962e-05\n",
      "  ('<s>', 'your', 'example', 'sentence') -> 1.6817576385431942e-05\n",
      "  ('your', 'example', 'sentence', 'goes') -> 0\n",
      "  ('example', 'sentence', 'goes', 'here') -> 0.00030047403141971737\n",
      "  ('sentence', 'goes', 'here', '.') -> 0.13059701492537312\n",
      "  ('goes', 'here', '.', '</s>') -> 1.1211717590287963e-06\n",
      "\n",
      "Sentence: Another test sentence for the backoff model.\n",
      "  ('<s>', '<s>', '<s>', 'another') -> 0.0005000426045268431\n",
      "  ('<s>', '<s>', 'another', 'test') -> 0\n",
      "  ('<s>', 'another', 'test', 'sentence') -> 1.6817576385431942e-05\n",
      "  ('another', 'test', 'sentence', 'for') -> 0.00907140070230199\n",
      "  ('test', 'sentence', 'for', 'the') -> 0.1635150166852058\n",
      "  ('sentence', 'for', 'the', 'backoff') -> 0\n",
      "  ('for', 'the', 'backoff', 'model') -> 0\n",
      "  ('the', 'backoff', 'model', '.') -> 0.02888138451258179\n",
      "  ('backoff', 'model', '.', '</s>') -> 1.1211717590287963e-06\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the Backoff Probability Function\n",
    "def test_backoff(sentences):\n",
    "    \"\"\"\n",
    "    Compute probabilities for given sentences using the backoff 4-gram model.\n",
    "    \"\"\"\n",
    "    for sentence in sentences:\n",
    "        # Preprocess sentence\n",
    "        tokens = word_tokenize(sentence.lower())\n",
    "        tokens = [\"<s>\", \"<s>\", \"<s>\"] + tokens + [\"</s>\"]\n",
    "        \n",
    "        # Create 4-grams\n",
    "        sentence_ngrams = list(ngrams(tokens, 4))\n",
    "        \n",
    "        # Compute probabilities\n",
    "        print(f\"Sentence: {sentence}\")\n",
    "        for ngram in sentence_ngrams:\n",
    "            prob = backoff_probability(ngram)\n",
    "            print(f\"  {ngram} -> {prob}\")\n",
    "        print()\n",
    "\n",
    "# Example Sentences for Testing\n",
    "test_sentences = [\n",
    "    \"Your example sentence goes here.\",\n",
    "    \"Another test sentence for the backoff model.\"\n",
    "]\n",
    "\n",
    "# Run the test\n",
    "test_backoff(test_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b). LM2: Interpolation method. The computation of each n-gram term is also smoothed using add-k smoothing technique. You can conduct your own experiments to find the best values for the hyper-parameters $\\lambda$'s and $k$ (use the validation set for the experiments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Lambda: (0.1, 0.2, 0.3, 0.4)\n",
      "Best k: 0.1\n",
      "Log-Probability: -690902.7502802992\n"
     ]
    }
   ],
   "source": [
    "# Add-k Smoothing Function\n",
    "def add_k_smoothing(ngram, ngram_counts, lower_order_counts, k, vocab_size):\n",
    "    \"\"\"\n",
    "    Compute add-k smoothed probability for an n-gram.\n",
    "    \"\"\"\n",
    "    ngram_count = ngram_counts[ngram]\n",
    "    prefix_count = lower_order_counts[ngram[:-1]] if len(ngram) > 1 else sum(unigram_counts.values())\n",
    "    return (ngram_count + k) / (prefix_count + k * vocab_size)\n",
    "\n",
    "\n",
    "# Interpolation Function\n",
    "def interpolated_probability(ngram, lambdas, k):\n",
    "    \"\"\"\n",
    "    Compute the interpolated probability of a word using 4-gram, 3-gram, 2-gram, and unigram models.\n",
    "    \"\"\"\n",
    "    lambda4, lambda3, lambda2, lambda1 = lambdas\n",
    "    p4 = add_k_smoothing(ngram, fourgram_counts, trigram_counts, k, vocab_size) if len(ngram) == 4 else 0\n",
    "    p3 = add_k_smoothing(ngram[1:], trigram_counts, bigram_counts, k, vocab_size) if len(ngram) >= 3 else 0\n",
    "    p2 = add_k_smoothing(ngram[2:], bigram_counts, unigram_counts, k, vocab_size) if len(ngram) >= 2 else 0\n",
    "    p1 = add_k_smoothing((ngram[-1],), unigram_counts, None, k, vocab_size)\n",
    "    return lambda4 * p4 + lambda3 * p3 + lambda2 * p2 + lambda1 * p1\n",
    "\n",
    "# Tune Hyperparameters Using the Validation Set\n",
    "def tune_hyperparameters(val_tokens, lambdas_list, k_values):\n",
    "    \"\"\"\n",
    "    Experiment with different values of lambdas and k using the validation set.\n",
    "    \"\"\"\n",
    "    best_lambda = None\n",
    "    best_k = None\n",
    "    best_log_prob = float('-inf')\n",
    "\n",
    "    for lambdas in lambdas_list:\n",
    "        for k in k_values:\n",
    "            # Compute log-probability on validation set\n",
    "            log_prob = 0\n",
    "            for ngram in ngrams(val_tokens, 4):\n",
    "                prob = interpolated_probability(ngram, lambdas, k)\n",
    "                log_prob += math.log(prob) if prob > 0 else float('-inf')\n",
    "            \n",
    "            # Update best parameters\n",
    "            if log_prob > best_log_prob:\n",
    "                best_log_prob = log_prob\n",
    "                best_lambda = lambdas\n",
    "                best_k = k\n",
    "\n",
    "    return best_lambda, best_k, best_log_prob\n",
    "\n",
    "# Example: Tune parameters\n",
    "lambda_options = [\n",
    "    (0.25, 0.25, 0.25, 0.25),  # Equal weights\n",
    "    (0.4, 0.3, 0.2, 0.1),      # Higher weight for 4-grams\n",
    "    (0.1, 0.2, 0.3, 0.4),      # Higher weight for unigrams\n",
    "]\n",
    "k_values = [0.1, 0.5, 1.0]\n",
    "\n",
    "best_lambda, best_k, best_log_prob = tune_hyperparameters(val_tokens, lambda_options, k_values)\n",
    "\n",
    "print(f\"Best Lambda: {best_lambda}\")\n",
    "print(f\"Best k: {best_k}\")\n",
    "print(f\"Log-Probability: {best_log_prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-gram: ('<s>', '<s>', '<s>', 'your'), Probability: 0.0009078306061606472\n",
      "N-gram: ('<s>', '<s>', 'your', 'test'), Probability: 7.328729239305256e-05\n",
      "N-gram: ('<s>', 'your', 'test', 'text'), Probability: 0.00011997286490295254\n",
      "N-gram: ('your', 'test', 'text', 'here'), Probability: 0.00024009509823971848\n",
      "N-gram: ('test', 'text', 'here', '</s>'), Probability: 9.950430777354939e-05\n"
     ]
    }
   ],
   "source": [
    "# Test the Model\n",
    "test_text = \"your test text here\"\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    return [\"<s>\", \"<s>\", \"<s>\"] + tokens + [\"</s>\"]\n",
    "test_tokens = preprocess_text(test_text)\n",
    "\n",
    "# Compute probabilities for test sentences\n",
    "for ngram in ngrams(test_tokens, 4):\n",
    "    prob = interpolated_probability(ngram, best_lambda, best_k)\n",
    "    print(f\"N-gram: {ngram}, Probability: {prob}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Evaluate the two models on the test set using perplexity evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of LM1 (Backoff Model): 2725941.8260661173\n",
      "Perplexity of LM2 (Interpolation + Add-k Smoothing): 5545.384438106823\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Perplexity Calculation Function\n",
    "def calculate_perplexity(test_tokens, model, lambdas=None, k=None):\n",
    "    \"\"\"\n",
    "    Calculate perplexity of a language model on the test set.\n",
    "    \n",
    "    Parameters:\n",
    "    - test_tokens: List of tokens in the test set.\n",
    "    - model: Function to compute n-gram probabilities.\n",
    "    - lambdas: Interpolation weights (only used for LM2).\n",
    "    - k: Smoothing parameter (only used for LM2).\n",
    "    \n",
    "    Returns:\n",
    "    - Perplexity value.\n",
    "    \"\"\"\n",
    "    log_prob_sum = 0\n",
    "    total_ngrams = 0\n",
    "\n",
    "    # Generate 4-grams from test tokens\n",
    "    test_ngrams = list(ngrams(test_tokens, 4))\n",
    "\n",
    "    for ngram in test_ngrams:\n",
    "        # Get the probability from the model\n",
    "        if lambdas and k:\n",
    "            # LM2 (interpolation with add-k smoothing)\n",
    "            prob = interpolated_probability(ngram, lambdas, k)\n",
    "        else:\n",
    "            # LM1 (backoff without smoothing)\n",
    "            prob = backoff_probability(ngram)\n",
    "\n",
    "        # If probability is 0, assign a small probability (to handle unknowns)\n",
    "        if prob == 0:\n",
    "            prob = 1e-10  # Small probability for unseen n-grams\n",
    "\n",
    "        # Accumulate log-probability\n",
    "        log_prob_sum += math.log(prob)\n",
    "        total_ngrams += 1\n",
    "\n",
    "    # Calculate perplexity\n",
    "    avg_log_prob = log_prob_sum / total_ngrams\n",
    "    perplexity = math.exp(-avg_log_prob)\n",
    "\n",
    "    return perplexity\n",
    "\n",
    "# Preprocess the Test Set\n",
    "# test_text = \"your test text here\"\n",
    "test_tokens = preprocess_text(test_text)  # Add <s>, </s> markers\n",
    "\n",
    "# Evaluate LM1 (Backoff Model)\n",
    "lm1_perplexity = calculate_perplexity(test_tokens, model=\"LM1\")\n",
    "print(f\"Perplexity of LM1 (Backoff Model): {lm1_perplexity}\")\n",
    "\n",
    "# Evaluate LM2 (Interpolation with Add-k Smoothing)\n",
    "lm2_perplexity = calculate_perplexity(test_tokens, model=\"LM2\", lambdas=best_lambda, k=best_k)\n",
    "print(f\"Perplexity of LM2 (Interpolation + Add-k Smoothing): {lm2_perplexity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Create a text generator using each of the two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated token:  ['<s>', '<s>', '<s>', 'of']\n",
      "generated token:  ['<s>', '<s>', '<s>', 'of', 'his']\n",
      "generated token:  ['<s>', '<s>', '<s>', 'of', 'his', 'learned']\n",
      "generated token:  ['<s>', '<s>', '<s>', 'of', 'his', 'learned', 'and']\n",
      "generated token:  ['<s>', '<s>', '<s>', 'of', 'his', 'learned', 'and', 'hast']\n",
      "generated token:  ['<s>', '<s>', '<s>', 'of', 'his', 'learned', 'and', 'hast', 'given']\n",
      "generated token:  ['<s>', '<s>', '<s>', 'of', 'his', 'learned', 'and', 'hast', 'given', 'him']\n",
      "generated token:  ['<s>', '<s>', '<s>', 'of', 'his', 'learned', 'and', 'hast', 'given', 'him', '.']\n",
      "generated token:  ['<s>', '<s>', '<s>', 'of', 'his', 'learned', 'and', 'hast', 'given', 'him', '.', 'and']\n",
      "generated token:  ['<s>', '<s>', '<s>', 'of', 'his', 'learned', 'and', 'hast', 'given', 'him', '.', 'and', 'power']\n",
      "generated token:  ['<s>', '<s>', '<s>', 'of', 'his', 'learned', 'and', 'hast', 'given', 'him', '.', 'and', 'power', 'at']\n",
      "generated token:  ['<s>', '<s>', '<s>', 'of', 'his', 'learned', 'and', 'hast', 'given', 'him', '.', 'and', 'power', 'at', 'all']\n",
      "generated token:  ['<s>', '<s>', '<s>', 'of', 'his', 'learned', 'and', 'hast', 'given', 'him', '.', 'and', 'power', 'at', 'all', 'to']\n",
      "generated token:  ['<s>', '<s>', '<s>', 'of', 'his', 'learned', 'and', 'hast', 'given', 'him', '.', 'and', 'power', 'at', 'all', 'to', 'abraham']\n",
      "generated token:  ['<s>', '<s>', '<s>', 'of', 'his', 'learned', 'and', 'hast', 'given', 'him', '.', 'and', 'power', 'at', 'all', 'to', 'abraham', ',']\n",
      "generated token:  ['<s>', '<s>', '<s>', 'of', 'his', 'learned', 'and', 'hast', 'given', 'him', '.', 'and', 'power', 'at', 'all', 'to', 'abraham', ',', 'impression']\n",
      "generated token:  ['<s>', '<s>', '<s>', 'of', 'his', 'learned', 'and', 'hast', 'given', 'him', '.', 'and', 'power', 'at', 'all', 'to', 'abraham', ',', 'impression', 'and']\n",
      "Generated Text (LM1 - Backoff):\n",
      "of his learned and hast given him . and power at all to abraham , impression and\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Step 1: Text Generator for LM1 (Backoff Model)\n",
    "def generate_text_lm1(start_tokens, max_length=20):\n",
    "    \"\"\"\n",
    "    Generate text using LM1 (Backoff Model).\n",
    "    \n",
    "    Parameters:\n",
    "    - start_tokens: List of tokens to start the text generation (e.g., [\"<s>\", \"<s>\", \"<s>\"]).\n",
    "    - max_length: Maximum length of the generated sequence.\n",
    "    \n",
    "    Returns:\n",
    "    - Generated text as a string.\n",
    "    \"\"\"\n",
    "    generated_tokens = start_tokens[:]\n",
    "    # print(\"generated token: \", generated_tokens)\n",
    "    while len(generated_tokens) < max_length:\n",
    "        context = tuple(generated_tokens[-3:])  # Use the last 3 tokens as context for 4-grams\n",
    "        next_token_probs = {}\n",
    "\n",
    "        # Compute probabilities for the next token\n",
    "        for token in unigram_counts.keys():\n",
    "           \n",
    "            ngram = context + token#(token,)\n",
    "            # print(\"ngram: \", ngram)\n",
    "            prob = backoff_probability(ngram)  # Use LM1's backoff probability function\n",
    "            if prob > 0:\n",
    "                next_token_probs[token] = prob\n",
    "\n",
    "        # print(\"next token prob: \", next_token_probs)\n",
    "        # Normalize probabilities\n",
    "        total_prob = sum(next_token_probs.values())\n",
    "        normalized_probs = {token: prob / total_prob for token, prob in next_token_probs.items()}\n",
    "        # print(\"normalise prob: \", normalized_probs)\n",
    "        # Sample the next token\n",
    "        next_token = random.choices(list(normalized_probs.keys()), weights=normalized_probs.values())[0]\n",
    "        # print(\"next token: \", next_token)\n",
    "        # Stop if end token is generated\n",
    "        if next_token == \"</s>\":\n",
    "            break\n",
    "        \n",
    "        generated_tokens.extend(list(next_token))\n",
    "        print(\"generated token: \", generated_tokens)\n",
    "\n",
    "    return \" \".join(generated_tokens[3:])  # Remove the initial <s> tokens\n",
    "\n",
    "\n",
    "# Generate Text\n",
    "start_tokens = [\"<s>\", \"<s>\", \"<s>\"]\n",
    "\n",
    "# Generate text using LM1\n",
    "generated_text_lm1 = generate_text_lm1(start_tokens)\n",
    "print(\"Generated Text (LM1 - Backoff):\")\n",
    "print(generated_text_lm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text (LM2 - Interpolation + Add-k Smoothing):\n",
      "1:22 weakness hollow 26:22 presence 5:2 difference narrow tempted secrets sepulchre contained keep overthrown ascended 2:14 weighed\n"
     ]
    }
   ],
   "source": [
    "# Text Generator for LM2 (Interpolation + Add-k Smoothing)\n",
    "def generate_text_lm2(start_tokens, lambdas, k, max_length=20):\n",
    "    \"\"\"\n",
    "    Generate text using LM2 (Interpolation + Add-k Smoothing).\n",
    "    \n",
    "    Parameters:\n",
    "    - start_tokens: List of tokens to start the text generation (e.g., [\"<s>\", \"<s>\", \"<s>\"]).\n",
    "    - lambdas: Interpolation weights.\n",
    "    - k: Add-k smoothing parameter.\n",
    "    - max_length: Maximum length of the generated sequence.\n",
    "    \n",
    "    Returns:\n",
    "    - Generated text as a string.\n",
    "    \"\"\"\n",
    "    generated_tokens = start_tokens[:]\n",
    "\n",
    "    while len(generated_tokens) < max_length:\n",
    "        context = tuple(generated_tokens[-3:])  # Use the last 3 tokens as context for 4-grams\n",
    "        next_token_probs = {}\n",
    "\n",
    "        # Compute probabilities for the next token\n",
    "        for token in unigram_counts.keys():\n",
    "            ngram = context + token#(token,)\n",
    "            prob = interpolated_probability(ngram, lambdas, k)  # Use LM2's interpolated probability\n",
    "            if prob > 0:\n",
    "                next_token_probs[token] = prob\n",
    "\n",
    "        # Normalize probabilities\n",
    "        total_prob = sum(next_token_probs.values())\n",
    "        normalized_probs = {token: prob / total_prob for token, prob in next_token_probs.items()}\n",
    "\n",
    "        # Sample the next token\n",
    "        next_token = random.choices(list(normalized_probs.keys()), weights=normalized_probs.values())[0]\n",
    "\n",
    "        # Stop if end token is generated\n",
    "        if next_token == \"</s>\":\n",
    "            break\n",
    "\n",
    "        # generated_tokens.append(next_token)\n",
    "        generated_tokens.extend(list(next_token))\n",
    "\n",
    "    return \" \".join(generated_tokens[3:])  # Remove the initial <s> tokens\n",
    "\n",
    "\n",
    "# Generate text using LM2\n",
    "best_lambda = (0.4, 0.3, 0.2, 0.1)  # Example best lambdas (from tuning)\n",
    "best_k = 0.5  # Example best k (from tuning)\n",
    "generated_text_lm2 = generate_text_lm2(start_tokens, best_lambda, best_k)\n",
    "print(\"\\nGenerated Text (LM2 - Interpolation + Add-k Smoothing):\")\n",
    "print(generated_text_lm2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
